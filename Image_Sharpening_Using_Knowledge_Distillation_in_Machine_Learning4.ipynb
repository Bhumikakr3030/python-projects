{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyNDPhFCUJUei0jvgThiZcTc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhumikakr3030/python-projects/blob/main/Image_Sharpening_Using_Knowledge_Distillation_in_Machine_Learning4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Knowledge Distillation**\n",
        "\n",
        "Knowledge Distillation is a procedure for model compression, in which a small (student) model is trained to match a large pre-trained (teacher) model. Knowledge is transferred from the teacher model to the student by minimizing a loss function, aimed at matching softened teacher logits as well as ground-truth labels.\n",
        "\n",
        "The logits are softened by applying a \"temperature\" scaling function in the softmax, effectively smoothing out the probability distribution and revealing inter-class relationships learned by the teacher.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_enhPpIspyJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Overview**\n",
        "\n",
        "#**Key Concepts in This Implementation**\n",
        "\n",
        "\n",
        "*   Teacher Model: A more complex network that learns to sharpen images effectively\n",
        "\n",
        "*   Student Model: A simpler network that learns to mimic the teacher's behavior\n",
        "\n",
        "*   Alpha Parameter: Balances between ground truth and teacher supervision\n",
        "\n",
        "*   KL divergence loss between student and teacher outputs\n",
        "\n",
        "*   Knowledge Distillation Loss: Combines.\n",
        "\n",
        "*   Traditional MSE loss between student output and ground truth\n",
        "\n",
        "*   Temperature Parameter: Controls how much we soften the probability distributions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k1NWh5YxvQHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python matplotlib tensorflow\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n"
      ],
      "metadata": {
        "id": "pniSgTPLxlLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from google.colab import files\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Step 2: Dataset\n",
        "class SharpeningDataset(Dataset):\n",
        "    def __init__(self, image_paths, blur_radius=3, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.blur_radius = blur_radius\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        if img_path.startswith(\"http\"):\n",
        "            response = requests.get(img_path)\n",
        "            sharp_img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "        else:\n",
        "            sharp_img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        sharp_np = np.array(sharp_img)\n",
        "        blurred_np = cv2.GaussianBlur(sharp_np, (self.blur_radius, self.blur_radius), 0)\n",
        "        blurred_img = Image.fromarray(blurred_np)\n",
        "\n",
        "        if self.transform:\n",
        "            sharp_img = self.transform(sharp_img)\n",
        "            blurred_img = self.transform(blurred_img)\n",
        "\n",
        "        return blurred_img, sharp_img\n",
        "\n",
        "# Transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Upload images\n",
        "print(\"Upload images for training (sharp ground truths):\")\n",
        "uploaded = files.upload()\n",
        "image_paths = list(uploaded.keys())\n",
        "\n",
        "dataset = SharpeningDataset(image_paths, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Step 3: Models\n",
        "class TeacherModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TeacherModel, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2), nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2), nn.ReLU(),\n",
        "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(self.encoder(x))\n",
        "\n",
        "class StudentModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StudentModel, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 3, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "teacher = TeacherModel().to(device)\n",
        "student = StudentModel().to(device)\n",
        "\n",
        "# Step 4: Knowledge Distillation Training\n",
        "def train_student_with_distillation(epochs=10, alpha=0.7, temperature=2.0):\n",
        "    criterion_mse = nn.MSELoss()\n",
        "    criterion_kl = nn.KLDivLoss(reduction='batchmean')\n",
        "    optimizer = optim.Adam(student.parameters(), lr=0.001)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "    teacher.eval()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for blurred, sharp in dataloader:\n",
        "            blurred, sharp = blurred.to(device), sharp.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher(blurred)\n",
        "\n",
        "            student_logits = student(blurred)\n",
        "\n",
        "            loss_mse = criterion_mse(student_logits, sharp)\n",
        "\n",
        "            T = temperature\n",
        "            soft_teacher = nn.functional.softmax(teacher_logits / T, dim=1)\n",
        "            soft_student = nn.functional.log_softmax(student_logits / T, dim=1)\n",
        "\n",
        "            B, C, H, W = student_logits.shape\n",
        "            soft_teacher = soft_teacher.permute(0, 2, 3, 1).reshape(-1, C)\n",
        "            soft_student = soft_student.permute(0, 2, 3, 1).reshape(-1, C)\n",
        "\n",
        "            loss_kl = criterion_kl(soft_student, soft_teacher) * (T ** 2)\n",
        "\n",
        "            loss = alpha * loss_mse + (1 - alpha) * loss_kl\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Total Loss: {loss.item():.4f} | MSE: {loss_mse.item():.4f} | KL: {loss_kl.item():.4f}\")\n",
        "\n",
        "# Step 5: Visualization\n",
        "def denormalize(tensor):\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
        "    return torch.clamp(tensor * std + mean, 0, 1)\n",
        "\n",
        "def visualize_results(num_images=3):\n",
        "    teacher.eval()\n",
        "    student.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (blurred, sharp) in enumerate(dataloader):\n",
        "            if i >= num_images:\n",
        "                break\n",
        "            blurred, sharp = blurred.to(device), sharp.to(device)\n",
        "            out_student = student(blurred)\n",
        "            out_teacher = teacher(blurred)\n",
        "\n",
        "            # Plot\n",
        "            fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "            images = [blurred[0], out_student[0], out_teacher[0], sharp[0]]\n",
        "            titles = ['Blurred Input', 'Student Output', 'Teacher Output', 'Ground Truth']\n",
        "\n",
        "            for ax, img, title in zip(axes, images, titles):\n",
        "                ax.imshow(denormalize(img).permute(1, 2, 0).cpu().numpy())\n",
        "                ax.set_title(title)\n",
        "                ax.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "# Step 6: Run\n",
        "train_student_with_distillation(epochs=10)\n",
        "visualize_results()\n",
        "\n",
        "# Save models\n",
        "torch.save(teacher.state_dict(), \"teacher_blur2sharp.pth\")\n",
        "torch.save(student.state_dict(), \"student_blur2sharp.pth\")\n"
      ],
      "metadata": {
        "id": "_j1O8ZgT0GcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Set up environment and imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Step 2: Data Preparation\n",
        "class SharpeningDataset(Dataset):\n",
        "    def __init__(self, image_paths=None, blur_radius=3, transform=None):\n",
        "        self.image_paths = image_paths or []\n",
        "        self.blur_radius = blur_radius\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image (works with both local files and URLs)\n",
        "        if isinstance(self.image_paths[idx], str) and self.image_paths[idx].startswith('http'):\n",
        "            response = requests.get(self.image_paths[idx])\n",
        "            sharp_img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "        else:\n",
        "            sharp_img = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "\n",
        "        # Create blurred version\n",
        "        sharp_np = np.array(sharp_img)\n",
        "        blurred_np = cv2.GaussianBlur(sharp_np, (self.blur_radius, self.blur_radius), 0)\n",
        "        blurred_img = Image.fromarray(blurred_np)\n",
        "\n",
        "        if self.transform:\n",
        "            sharp_img = self.transform(sharp_img)\n",
        "            blurred_img = self.transform(blurred_img)\n",
        "\n",
        "        return blurred_img, sharp_img\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load data (manual upload in Colab)\n",
        "print(\"Please upload your images:\")\n",
        "uploaded = files.upload()\n",
        "image_paths = list(uploaded.keys())\n",
        "\n",
        "if not image_paths:\n",
        "    raise ValueError(\"No images uploaded! Please try again.\")\n",
        "\n",
        "dataset = SharpeningDataset(image_paths, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Step 3: Model Definitions\n",
        "class TeacherModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TeacherModel, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(self.encoder(x))\n",
        "\n",
        "class StudentModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StudentModel, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 3, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "teacher = TeacherModel().to(device)\n",
        "student = StudentModel().to(device)\n",
        "\n",
        "# Step 4: Training with Knowledge Distillation\n",
        "def train_student_with_distillation(epochs=10, temperature=2.0, alpha=0.7):\n",
        "    criterion_mse = nn.MSELoss()\n",
        "    criterion_kl = nn.KLDivLoss(reduction='batchmean')\n",
        "    optimizer = optim.Adam(student.parameters(), lr=0.001)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "    teacher.eval()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for blurred, sharp in dataloader:\n",
        "            blurred, sharp = blurred.to(device), sharp.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher(blurred)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            student_logits = student(blurred)\n",
        "\n",
        "            # MSE loss\n",
        "            loss_mse = criterion_mse(student_logits, sharp)\n",
        "\n",
        "            # KL divergence loss\n",
        "            T = temperature\n",
        "            soft_teacher = nn.functional.softmax(teacher_logits / T, dim=1)\n",
        "            soft_student = nn.functional.log_softmax(student_logits / T, dim=1)\n",
        "\n",
        "            # Reshape for KLDivLoss\n",
        "            B, C, H, W = student_logits.shape\n",
        "            soft_teacher = soft_teacher.permute(0, 2, 3, 1).reshape(-1, C)\n",
        "            soft_student = soft_student.permute(0, 2, 3, 1).reshape(-1, C)\n",
        "\n",
        "            loss_kl = criterion_kl(soft_student, soft_teacher) * (T ** 2)\n",
        "\n",
        "            # Combined loss\n",
        "            loss = alpha * loss_mse + (1 - alpha) * loss_kl\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f} (MSE: {loss_mse.item():.4f}, KL: {loss_kl.item():.4f})\")\n",
        "\n",
        "# Step 5: Evaluation and Visualization\n",
        "def denormalize(tensor):\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
        "    return torch.clamp(tensor * std + mean, 0, 1)\n",
        "\n",
        "def visualize_results(num_images=3):\n",
        "    student.eval()\n",
        "    teacher.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (blurred, sharp) in enumerate(dataloader):\n",
        "            if i >= num_images:\n",
        "                break\n",
        "\n",
        "            blurred, sharp = blurred.to(device), sharp.to(device)\n",
        "            student_output = student(blurred)\n",
        "            teacher_output = teacher(blurred)\n",
        "\n",
        "            # Denormalize images\n",
        "            blurred_img = denormalize(blurred[0]).cpu().permute(1, 2, 0).numpy()\n",
        "            sharp_img = denormalize(sharp[0]).cpu().permute(1, 2, 0).numpy()\n",
        "            student_img = denormalize(student_output[0]).cpu().permute(1, 2, 0).numpy()\n",
        "            teacher_img = denormalize(teacher_output[0]).cpu().permute(1, 2, 0).numpy()\n",
        "\n",
        "            # Plot comparison\n",
        "            plt.figure(figsize=(20, 5))\n",
        "            titles = ['Blurred Input', 'Student Output', 'Teacher Output', 'Ground Truth']\n",
        "            images = [blurred_img, student_img, teacher_img, sharp_img]\n",
        "\n",
        "            for j in range(4):\n",
        "                plt.subplot(1, 4, j+1)\n",
        "                plt.imshow(images[j])\n",
        "                plt.title(titles[j])\n",
        "                plt.axis('off')\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "# Step 6: Run Training and Evaluation\n",
        "if __name__ == \"__main__\":\n",
        "    # Train teacher first (optional)\n",
        "    # train_teacher()  # You would need to implement this\n",
        "\n",
        "    # Train student with distillation\n",
        "    train_student_with_distillation(epochs=10)\n",
        "\n",
        "    # Save models\n",
        "    torch.save(teacher.state_dict(), \"teacher.pth\")\n",
        "    torch.save(student.state_dict(), \"student.pth\")\n",
        "\n",
        "    # Visualize results\n",
        "    visualize_results()"
      ],
      "metadata": {
        "id": "0eKNfkX4u10Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shared\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PyTorch (Image sharpening)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Keras (MNIST classification)\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n"
      ],
      "metadata": {
        "id": "KDInOYur0qGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined Knowledge Distillation Implementation with PyTorch and Keras\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import requests\n",
        "from google.colab import files\n",
        "\n",
        "# Keras imports\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ==================================================================\n",
        "# PyTorch Implementation - Image Sharpening with Knowledge Distillation\n",
        "# ==================================================================\n",
        "\n",
        "class SharpeningDataset(Dataset):\n",
        "    def __init__(self, image_paths=None, blur_radius=3, transform=None):\n",
        "        self.image_paths = image_paths or []\n",
        "        self.blur_radius = blur_radius\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image (works with both local files and URLs)\n",
        "        if isinstance(self.image_paths[idx], str) and self.image_paths[idx].startswith('http'):\n",
        "            response = requests.get(self.image_paths[idx])\n",
        "            sharp_img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "        else:\n",
        "            sharp_img = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "\n",
        "        # Create blurred version\n",
        "        sharp_np = np.array(sharp_img)\n",
        "        blurred_np = cv2.GaussianBlur(sharp_np, (self.blur_radius, self.blur_radius), 0)\n",
        "        blurred_img = Image.fromarray(blurred_np)\n",
        "\n",
        "        if self.transform:\n",
        "            sharp_img = self.transform(sharp_img)\n",
        "            blurred_img = self.transform(blurred_img)\n",
        "\n",
        "        return blurred_img, sharp_img\n",
        "\n",
        "class TeacherModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TeacherModel, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(self.encoder(x))\n",
        "\n",
        "class StudentModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StudentModel, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 3, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def pytorch_knowledge_distillation():\n",
        "    # Define transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Load data (manual upload in Colab)\n",
        "    print(\"Please upload your images:\")\n",
        "    uploaded = files.upload()\n",
        "    image_paths = list(uploaded.keys())\n",
        "\n",
        "    if not image_paths:\n",
        "        raise ValueError(\"No images uploaded! Please try again.\")\n",
        "\n",
        "    dataset = SharpeningDataset(image_paths, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "    # Initialize models\n",
        "    teacher = TeacherModel().to(device)\n",
        "    student = StudentModel().to(device)\n",
        "\n",
        "    # Training function\n",
        "    def train_student_with_distillation(epochs=10, temperature=2.0, alpha=0.7):\n",
        "        criterion_mse = nn.MSELoss()\n",
        "        criterion_kl = nn.KLDivLoss(reduction='batchmean')\n",
        "        optimizer = optim.Adam(student.parameters(), lr=0.001)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "        teacher.eval()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for blurred, sharp in dataloader:\n",
        "                blurred, sharp = blurred.to(device), sharp.to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    teacher_logits = teacher(blurred)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                student_logits = student(blurred)\n",
        "\n",
        "                # MSE loss\n",
        "                loss_mse = criterion_mse(student_logits, sharp)\n",
        "\n",
        "                # KL divergence loss\n",
        "                T = temperature\n",
        "                soft_teacher = nn.functional.softmax(teacher_logits / T, dim=1)\n",
        "                soft_student = nn.functional.log_softmax(student_logits / T, dim=1)\n",
        "\n",
        "                # Reshape for KLDivLoss\n",
        "                B, C, H, W = student_logits.shape\n",
        "                soft_teacher = soft_teacher.permute(0, 2, 3, 1).reshape(-1, C)\n",
        "                soft_student = soft_student.permute(0, 2, 3, 1).reshape(-1, C)\n",
        "\n",
        "                loss_kl = criterion_kl(soft_student, soft_teacher) * (T ** 2)\n",
        "\n",
        "                # Combined loss\n",
        "                loss = alpha * loss_mse + (1 - alpha) * loss_kl\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            scheduler.step()\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f} (MSE: {loss_mse.item():.4f}, KL: {loss_kl.item():.4f})\")\n",
        "\n",
        "    # Run training\n",
        "    train_student_with_distillation(epochs=10)\n",
        "\n",
        "    # Save models\n",
        "    torch.save(teacher.state_dict(), \"teacher.pth\")\n",
        "    torch.save(student.state_dict(), \"student.pth\")\n",
        "\n",
        "    # Visualization function\n",
        "    def denormalize(tensor):\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
        "        return torch.clamp(tensor * std + mean, 0, 1)\n",
        "\n",
        "    def visualize_results(num_images=3):\n",
        "        student.eval()\n",
        "        teacher.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (blurred, sharp) in enumerate(dataloader):\n",
        "                if i >= num_images:\n",
        "                    break\n",
        "\n",
        "                blurred, sharp = blurred.to(device), sharp.to(device)\n",
        "                student_output = student(blurred)\n",
        "                teacher_output = teacher(blurred)\n",
        "\n",
        "                # Denormalize images\n",
        "                blurred_img = denormalize(blurred[0]).cpu().permute(1, 2, 0).numpy()\n",
        "                sharp_img = denormalize(sharp[0]).cpu().permute(1, 2, 0).numpy()\n",
        "                student_img = denormalize(student_output[0]).cpu().permute(1, 2, 0).numpy()\n",
        "                teacher_img = denormalize(teacher_output[0]).cpu().permute(1, 2, 0).numpy()\n",
        "\n",
        "                # Plot comparison\n",
        "                plt.figure(figsize=(20, 5))\n",
        "                titles = ['Blurred Input', 'Student Output', 'Teacher Output', 'Ground Truth']\n",
        "                images = [blurred_img, student_img, teacher_img, sharp_img]\n",
        "\n",
        "                for j in range(4):\n",
        "                    plt.subplot(1, 4, j+1)\n",
        "                    plt.imshow(images[j])\n",
        "                    plt.title(titles[j])\n",
        "                    plt.axis('off')\n",
        "\n",
        "                plt.show()\n",
        "\n",
        "    visualize_results()\n",
        "\n",
        "# ==================================================================\n",
        "# Keras Implementation - MNIST Classification with Knowledge Distillation\n",
        "# ==================================================================\n",
        "\n",
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher):\n",
        "        super().__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.1,\n",
        "        temperature=3,\n",
        "    ):\n",
        "        super().compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def compute_loss(\n",
        "        self, x=None, y=None, y_pred=None, sample_weight=None, allow_empty=False\n",
        "    ):\n",
        "        teacher_pred = self.teacher(x, training=False)\n",
        "        student_loss = self.student_loss_fn(y, y_pred)\n",
        "\n",
        "        distillation_loss = self.distillation_loss_fn(\n",
        "            ops.softmax(teacher_pred / self.temperature, axis=1),\n",
        "            ops.softmax(y_pred / self.temperature, axis=1),\n",
        "        ) * (self.temperature**2)\n",
        "\n",
        "        loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "        return loss\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.student(x)\n",
        "\n",
        "def keras_knowledge_distillation():\n",
        "    # Create the teacher\n",
        "    teacher = keras.Sequential(\n",
        "        [\n",
        "            keras.Input(shape=(28, 28, 1)),\n",
        "            layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "            layers.LeakyReLU(negative_slope=0.2),\n",
        "            layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "            layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(10),\n",
        "        ],\n",
        "        name=\"teacher\",\n",
        "    )\n",
        "\n",
        "    # Create the student\n",
        "    student = keras.Sequential(\n",
        "        [\n",
        "            keras.Input(shape=(28, 28, 1)),\n",
        "            layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "            layers.LeakyReLU(negative_slope=0.2),\n",
        "            layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "            layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(10),\n",
        "        ],\n",
        "        name=\"student\",\n",
        "    )\n",
        "\n",
        "    # Clone student for later comparison\n",
        "    student_scratch = keras.models.clone_model(student)\n",
        "\n",
        "    # Prepare the dataset\n",
        "    batch_size = 64\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "    # Normalize data\n",
        "    x_train = x_train.astype(\"float32\") / 255.0\n",
        "    x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
        "\n",
        "    x_test = x_test.astype(\"float32\") / 255.0\n",
        "    x_test = np.reshape(x_test, (-1, 28, 28, 1))\n",
        "\n",
        "    # Train teacher\n",
        "    print(\"\\nTraining teacher model...\")\n",
        "    teacher.compile(\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "    teacher.fit(x_train, y_train, epochs=5)\n",
        "    teacher.evaluate(x_test, y_test)\n",
        "\n",
        "    # Distill teacher to student\n",
        "    print(\"\\nDistilling knowledge to student model...\")\n",
        "    distiller = Distiller(student=student, teacher=teacher)\n",
        "    distiller.compile(\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "        student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "        alpha=0.1,\n",
        "        temperature=10,\n",
        "    )\n",
        "    distiller.fit(x_train, y_train, epochs=3)\n",
        "    distiller.evaluate(x_test, y_test)\n",
        "\n",
        "    # Train student from scratch for comparison\n",
        "    print(\"\\nTraining student from scratch for comparison...\")\n",
        "    student_scratch.compile(\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "    student_scratch.fit(x_train, y_train, epochs=3)\n",
        "    student_scratch.evaluate(x_test, y_test)\n",
        "\n",
        "# ==================================================================\n",
        "# Main Execution\n",
        "# ==================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"PyTorch Image Sharpening with Knowledge Distillation\")\n",
        "    print(\"---------------------------------------------------\")\n",
        "    pytorch_knowledge_distillation()\n",
        "\n",
        "    print(\"\\nKeras MNIST Classification with Knowledge Distillation\")\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    keras_knowledge_distillation()"
      ],
      "metadata": {
        "id": "F-gByXj_1lIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What is MNIST Classification?**\n",
        "\n",
        "MNIST classification refers to a classic image classification task where a machine learning model learns to recognize handwritten digits from 0 to 9 using the MNIST dataset.\n",
        "MNIST stands for Modified National Institute of Standards and Technology.\n",
        "\n",
        "The MNIST (Modified National Institute of Standards and Technology) dataset is a widely used benchmark in machine learning and computer vision. It consists of 70,000 grayscale images (28×28 pixels) of handwritten digits (0–9), split into:\n",
        "\n",
        "60,000 training images\n",
        "\n",
        "10,000 test images\n",
        "It is a collection of 70,000 grayscale images of handwritten digits.\n",
        "\n",
        "60,000 images for training\n",
        "\n",
        "10,000 images for testing\n",
        "\n",
        "Each image:\n",
        "\n",
        "Size: 28 x 28 pixels\n",
        "\n",
        "Format: Single channel (grayscale)\n",
        "\n",
        "Label: A digit from 0 to 9"
      ],
      "metadata": {
        "id": "NttzKqUM6oUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Construct Distiller() class**\n",
        "\n",
        "The custom Distiller() class, overrides the Model methods compile, compute_loss, and call. In order to use the distiller, we need:\n",
        "\n",
        "A trained teacher model\n",
        "A student model to train\n",
        "A student loss function on the difference between student predictions and ground-truth\n",
        "A distillation loss function, along with a temperature, on the difference between the soft student predictions and the soft teacher labels\n",
        "An alpha factor to weight the student and distillation loss\n",
        "An optimizer for the student and (optional) metrics to evaluate performance\n",
        "In the compute_loss method, we perform a forward pass of both the teacher and student, calculate the loss with weighting of the student_loss and distillation_loss by alpha and 1 - alpha, respectively. Note: only the student weights are updated."
      ],
      "metadata": {
        "id": "-qaDAjsKqKpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "HZ8fXoC66noh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher):\n",
        "        super().__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.1,\n",
        "        temperature=3,\n",
        "    ):\n",
        "        \"\"\"Configure the distiller.\n",
        "\n",
        "        Args:\n",
        "            optimizer: Keras optimizer for the student weights\n",
        "            metrics: Keras metrics for evaluation\n",
        "            student_loss_fn: Loss function of difference between student\n",
        "                predictions and ground-truth\n",
        "            distillation_loss_fn: Loss function of difference between soft\n",
        "                student predictions and soft teacher predictions\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
        "            temperature: Temperature for softening probability distributions.\n",
        "                Larger temperature gives softer distributions.\n",
        "        \"\"\"\n",
        "        super().compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def compute_loss(\n",
        "        self, x=None, y=None, y_pred=None, sample_weight=None, allow_empty=False\n",
        "    ):\n",
        "        teacher_pred = self.teacher(x, training=False)\n",
        "        student_loss = self.student_loss_fn(y, y_pred)\n",
        "\n",
        "        distillation_loss = self.distillation_loss_fn(\n",
        "            ops.softmax(teacher_pred / self.temperature, axis=1),\n",
        "            ops.softmax(y_pred / self.temperature, axis=1),\n",
        "        ) * (self.temperature**2)\n",
        "\n",
        "        loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "        return loss\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.student(x)"
      ],
      "metadata": {
        "id": "vV2dmXZH63cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create student and teacher models**\n",
        "\n",
        "Initialy, we create a teacher model and a smaller student model. Both models are convolutional neural networks and created using Sequential(), but could be any Keras model."
      ],
      "metadata": {
        "id": "LQ3I_ZIrqZ_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the teacher\n",
        "teacher = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(negative_slope=0.2),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "        layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(10),\n",
        "    ],\n",
        "    name=\"teacher\",\n",
        ")\n",
        "\n",
        "# Create the student\n",
        "student = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(negative_slope=0.2),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(10),\n",
        "    ],\n",
        "    name=\"student\",\n",
        ")\n",
        "\n",
        "# Clone student for later comparison\n",
        "student_scratch = keras.models.clone_model(student)"
      ],
      "metadata": {
        "id": "_FWyzZal7Ewk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prepare the dataset**\n",
        "\n",
        "The dataset used for training the teacher and distilling the teacher is MNIST, and the procedure would be equivalent for any other dataset, e.g. CIFAR-10, with a suitable choice of models. Both the student and teacher are trained on the training set and evaluated on the test set."
      ],
      "metadata": {
        "id": "3KXO6VCbqjrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the train and test dataset.\n",
        "batch_size = 64\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize data\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
        "\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_test = np.reshape(x_test, (-1, 28, 28, 1))\n"
      ],
      "metadata": {
        "id": "fmMbQk-n7Gex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train the teacher**\n",
        "\n",
        "In knowledge distillation we assume that the teacher is trained and fixed. Thus, we start by training the teacher model on the training set in the usual way."
      ],
      "metadata": {
        "id": "FgNFA88Qqu_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Distill teacher to student**\n",
        "\n",
        "We have already trained the teacher model, and we only need to initialize a Distiller(student, teacher) instance, compile() it with the desired losses, hyperparameters and optimizer, and distill the teacher to the student."
      ],
      "metadata": {
        "id": "Y8mdwoVmqyYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train student from scratch for comparison**\n",
        "\n",
        "We can also train an equivalent student model from scratch without the teacher, in order to evaluate the performance gain obtained by knowledge distillation.\n",
        "\n",
        "If the teacher is trained for 5 full epochs and the student is distilled on this teacher for 3 full epochs, you should in this example experience a performance boost compared to training the same student model from scratch, and even compared to the teacher itself.\n",
        "\n",
        "We should expect the teacher to have accuracy around 97.6%, the student trained from scratch should be around 97.6%, and the distilled student should be around 98.1%. Remove or try out different seeds to use different weight initializations."
      ],
      "metadata": {
        "id": "z_6mlKHWrADt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train teacher as usual\n",
        "teacher.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train and evaluate teacher on data.\n",
        "teacher.fit(x_train, y_train, epochs=5)\n",
        "teacher.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "6fO46o8R7eJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and compile distiller\n",
        "distiller = Distiller(student=student, teacher=teacher)\n",
        "distiller.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "    alpha=0.1,\n",
        "    temperature=10,\n",
        ")\n",
        "\n",
        "# Distill teacher to student\n",
        "distiller.fit(x_train, y_train, epochs=3)\n",
        "\n",
        "# Evaluate student on test dataset\n",
        "distiller.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "X8nWoEJcQqai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train student as doen usually\n",
        "student_scratch.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train and evaluate student trained from scratch.\n",
        "student_scratch.fit(x_train, y_train, epochs=3)\n",
        "student_scratch.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "pdPJEYhkgFhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "lhvzBWskoG-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(model, x_test, y_test, title=\"Confusion Matrix\"):\n",
        "    y_pred_logits = model.predict(x_test)\n",
        "    y_pred = np.argmax(y_pred_logits, axis=1)\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(10))\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(f\"{title}\\nAccuracy: {acc:.4f}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Cm_XqahZoMpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(teacher, x_test, y_test, title=\"Teacher Model\")\n"
      ],
      "metadata": {
        "id": "kyWb4y6-oODw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(distiller, x_test, y_test, title=\"Distilled Student Model\")\n"
      ],
      "metadata": {
        "id": "bnor5-o0od0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(student_scratch, x_test, y_test, title=\"Student Trained from Scratch\")\n"
      ],
      "metadata": {
        "id": "3sIzJzE-oioT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(student_scratch, x_test, y_test, title=\"Student Trained from Scratch\")\n"
      ],
      "metadata": {
        "id": "CdB_TOk1oorP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy from evaluation outputs\n",
        "teacher_acc = 0.9760\n",
        "distilled_student_acc = 0.9692\n",
        "scratch_student_acc = 0.9737\n",
        "\n",
        "# Bar graph\n",
        "models = ['Teacher', 'Distilled Student', 'Scratch Student']\n",
        "accuracies = [teacher_acc, distilled_student_acc, scratch_student_acc]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(models, accuracies, color=['blue', 'green', 'orange'])\n",
        "plt.ylim(0.95, 0.99)\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xaNLx-4EotPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use real values from model.evaluate() results\n",
        "teacher_acc = 0.9781\n",
        "distilled_student_acc = 0.9692\n",
        "scratch_student_acc = 0.9778\n",
        "\n",
        "# Accuracy bar plot\n",
        "labels = [\"Teacher\", \"Distilled Student\", \"Scratch Student\"]\n",
        "accuracies = [teacher_acc, distilled_student_acc, scratch_student_acc]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(labels, accuracies, color=[\"skyblue\", \"lightgreen\", \"orange\"])\n",
        "plt.ylim(0.94, 0.99)\n",
        "plt.title(\"Knowledge Distillation Accuracy Comparison\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, height, f'{height:.4f}', ha='center', va='bottom')\n",
        "\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qJTIWs7SpS5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Accuracy values from evaluations\n",
        "teacher_acc = 0.9781\n",
        "distilled_student_acc = 0.9692\n",
        "scratch_student_acc = 0.9778\n",
        "\n",
        "# X and Y values\n",
        "models = [\"Teacher\", \"Distilled Student\", \"Scratch Student\"]\n",
        "accuracies = [teacher_acc, distilled_student_acc, scratch_student_acc]\n",
        "\n",
        "# Line plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(models, accuracies, marker='o', linestyle='-', color='blue', linewidth=2, markersize=8)\n",
        "\n",
        "# Annotate accuracy values\n",
        "for i, acc in enumerate(accuracies):\n",
        "    plt.text(i, acc + 0.001, f\"{acc:.4f}\", ha='center', fontsize=10)\n",
        "\n",
        "plt.ylim(0.94, 0.99)\n",
        "plt.title(\"Knowledge Distillation Accuracy Comparison\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QuT7MxFZpiEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bhumika KR  1NT22EC036\n",
        "# Deepika P   1NT23CS057\n",
        "#Samitha NS   1NT22EC099"
      ],
      "metadata": {
        "id": "EoB7gf7hknwq"
      }
    }
  ]
}